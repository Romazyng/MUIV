{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bFSsOWiLMb4o",
        "outputId": "db4a3061-800d-4051-9ec9-7b54aa1b2477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 989 entries, 0 to 999\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Название        989 non-null    object\n",
            " 1   Время чтения    989 non-null    object\n",
            " 2   Просмотры       989 non-null    object\n",
            " 3   Текст           989 non-null    object\n",
            " 4   Ключевые слова  989 non-null    object\n",
            " 5   Ссылка          989 non-null    object\n",
            "dtypes: object(6)\n",
            "memory usage: 54.1+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/habr_articles.csv\")\n",
        "df =  df.dropna(subset=[\"Текст\"])\n",
        "df.info()\n",
        "text = df['Текст']\n",
        "input_sequences = []\n",
        "\n",
        "def clean(text):\n",
        "    text = re.sub(r'[!-,.«»:?;]', '', text.lower())\n",
        "    text = re.sub(r'\\s+', ' ',text)\n",
        "    return text\n",
        "\n",
        "text = text.apply(clean)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "for line in text:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences,\n",
        "                                maxlen=max_sequence_length,\n",
        "                                padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lx7QQ_WMb4s"
      },
      "source": [
        "# Нейросетки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TaJgrvJMb4v"
      },
      "outputs": [],
      "source": [
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(total_words, activation='softmax'))\n",
        "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_rnn.fit(X, y, epochs=10, verbose=0)\n",
        "\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model_gru.add(GRU(100))\n",
        "model_gru.add(Dense(total_words, activation='softmax'))\n",
        "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_gru.fit(X, y, epochs=10, verbose=0)\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model_lstm.add(LSTM(100))\n",
        "model_lstm.add(Dense(total_words, activation='softmax'))\n",
        "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_lstm.fit(X, y, epochs=10, verbose=0)\n",
        "\n",
        "def generate_text(seed_text, model, max_sequence_len, num_words):\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = pd.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "generated_text_rnn = generate_text(\"Это\", model_rnn, max_sequence_length, num_words=10)\n",
        "generated_text_gru = generate_text(\"Это\", model_gru, max_sequence_length, num_words=10)\n",
        "generated_text_lstm = generate_text(\"Это\", model_lstm, max_sequence_length, num_words=10)\n",
        "\n",
        "print(\"Сгенерированный текст (SimpleRNN):\", generated_text_rnn)\n",
        "print(\"Сгенерированный текст (GRU):\", generated_text_gru)\n",
        "print(\"Сгенерированный текст (LSTM):\", generated_text_lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAblu8SjMb4w"
      },
      "source": [
        "Статистическая модель марковская цепь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bdsak3oYMb4y",
        "outputId": "4afb004e-71b2-4c41-b9a5-697abba554dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст 1:\n",
            "данных используемых для повышения безопасности ит-систем северной кореи велкам\n",
            "\n",
            "Текст 2:\n",
            "по 4 уровню доверия\n",
            "\n",
            "Текст 3:\n",
            "вроде ci/cd точечное внедрение инструментов и фреймворков исходя из меняющегося контекста по заранее определенным правилам особенность именно безлимитной версии покера в том что её проектируют в первую очередь опирается на личный опыт вхождения в it и представить не мог представить что эта тема привлекает внимание исследователей в области автоматизации объясню просто как\n",
            "\n",
            "Текст 4:\n",
            "к решению задач кибербезопасности приводят к созданию бота\n",
            "\n",
            "Текст 5:\n",
            "систем 1с в качестве самого сервиса будем использовать nextjs shadcn для интерфейса и sms api с подтверждением договоренностей и следующими шагами\n",
            "\n",
            "Текст 6:\n",
            "мер мотивации недостаток практики и скучная слишком сухо преподнесенная теория – типичные примеры может показаться сложной задачей но если запустить их ещё в dosbox у себя на машине можно - но покажем ассемблеры для нескольких популярных архитектур микроконтроллеров arm32 avr msp430 8051 - и да речь в книге идет о нескольких серверах\n",
            "\n",
            "Текст 7:\n",
            "хочу затронуть феномен покера для которого создается ai и формальных методов в процессе адаптации этих интерфейсов на основе текущих данных таких как загрузка cpu и память заметно влияла на время автономной работы микропотребляющего устройства на собственную операционную систему ос это может показаться что это значит rest api один из базовых инструментов необходимых\n",
            "\n",
            "Текст 8:\n",
            "серьезный вызов результатом мы считали не размер вознаграждения а сам процесс обучения и нейронных сетей к примеру если мы учим llm понимать и синтезировать сложные наборы данных и автоматизированные реакции на изменения начнут завтра где-нибудь выдавать кредиты на развитие сюжета мы рассмотрим несколько методов оптимизации используемых для повышения производительности cnn на примере\n",
            "\n",
            "Текст 9:\n",
            "только начинали понимать как сильно может измениться процесс создания софта но и для меня так как это помогает в практике разработки игр\n",
            "\n",
            "Текст 10:\n",
            "мы нашли несмотря на их принципиальное сходство и подробно опишу нашу архитектуру обработки логов\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/habr_articles.csv\")\n",
        "df = df.dropna(subset=[\"Текст\"])\n",
        "df = df.drop(columns=['Название', 'Время чтения', 'Просмотры', 'Ключевые слова', 'Ссылка'])\n",
        "text = df['Текст']\n",
        "\n",
        "def clean(text):\n",
        "    text = re.sub(r'[!-,.«»:?;—]', '', text.lower())\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "texts = text.apply(clean)\n",
        "\n",
        "def build_markov_chain(texts, n=2):\n",
        "    chain = defaultdict(list)\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            key = tuple(tokens[i:i + n - 1])\n",
        "            next_word = tokens[i + n - 1]\n",
        "            chain[key].append(next_word)\n",
        "    return chain\n",
        "\n",
        "def generate_text_markov(chain, n=2, start_text='', max_words=50):\n",
        "    if start_text:\n",
        "        current_state = tuple(start_text.split()[:n-1])\n",
        "    else:\n",
        "        current_state = random.choice(list(chain.keys()))\n",
        "\n",
        "    generated_text = list(current_state)\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        next_words = chain.get(current_state, [])\n",
        "        if not next_words:\n",
        "            break\n",
        "        next_word = random.choice(next_words)\n",
        "        generated_text.append(next_word)\n",
        "        current_state = tuple(generated_text[-(n-1):])\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "def generate_multiple_texts(chain, n=2, max_words=50, number_offers=10):\n",
        "    texts = []\n",
        "    for _ in range(number_offers):\n",
        "        generated_text = generate_text_markov(chain, n=n, start_text='', max_words=max_words)\n",
        "        texts.append(generated_text)\n",
        "    return texts\n",
        "\n",
        "\n",
        "n = 3\n",
        "markov_chain = build_markov_chain(texts, n)\n",
        "generated_texts = generate_multiple_texts(markov_chain, n=n, max_words=50, number_offers=10)\n",
        "\n",
        "for i, t in enumerate(generated_texts, 1):\n",
        "    print(f\"Текст {i}:\\n{t}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}